# Mechine Learning

**Supervised Learning 监督学习**

预测给定输入的“正确”输出

**Unsupervised Learning 无监督学习**

识别未标记数据集的内在模式、依赖关系和相关性

**Reinforcement Learning 强化学习**

通过反复试错来训练模型，使其能够评估环境并采取获得最大奖励的行动，适合没有唯一真值，但存在好行动和坏行动的场景

## 一、机器学习算法

### 1、监督学习 

监督学习常用于模型训练，完成分类、回归任务

#### 1.1、常用回归算法

[**线性回归**](https://www.ibm.com/think/topics/linear-regression)是机器学习中最基础且应用最广泛的算法之一。线性回归算法将输出（因变量）计算为一个或多个输入变量（自变量）的加权组合。通过学习每个输入变量的最佳权重，该算法可以为训练数据点生成“最佳拟合线”。线性回归有许多变体和扩展，它们使用类似的逻辑来模拟因变量和自变量之间更复杂的数学关系，例如*多项式回归* （用于模拟非线性、曲线关系）或*分位数回归* （用于模拟数据集分布中特定点的关系）。

[**决策树**](https://www.ibm.com/think/topics/decision-trees)**算法**通过一系列分支的 if-then-else 决策来得出最终的输出预测，这些决策可以可视化为树状图。它们既可用于回归，也可用于分类。与大多数监督学习算法不同，该算法的目标不是通过最小化单个全局损失函数来优化其输出预测，而是优化树中每个分支节点的预测能力。

[**状态空间模型（SSM）**](https://www.ibm.com/think/topics/state-space-model) 通过两个相互关联的方程来模拟动态系统和序列数据：一个是*状态方程，它*描述系统内部不可直接观测的动态特性（“状态”）；另一个是*输出方程，它*描述这些内部动态特性如何与可观测的结果（即系统输出）相关联。状态空间模型的应用领域十分广泛，涵盖了从电气工程到金融预测再到 自然语言处理（NLP）等诸多领域。

### 1.2、常用分类算法

[**朴素贝叶斯**](http://www.ibm.com/think/topics/naive-bayes)分类器基于[贝叶斯定理](https://ibm.com/topics/naive-bayes)的逻辑运行，该定理本质上是将后续事件（例如结果）的信息用于更新对早期事件（例如输入）的理解这一思想的数学表述。换句话说，该模型根据给定输入变量与特定结果的相关性强弱来学习其相对重要性。其“朴素”假设是所有用于分类的特征彼此独立。这种简化使得该算法对于垃圾邮件检测等简单任务而言快速高效。

[**逻辑回归**](https://www.ibm.com/think/topics/logistic-regression)通过将输入特征的加权和输入到 [sigmoid 函数](http://community.ibm.com/community/user/ai-datascience/blogs/pavan-saish-naru/2023/01/27/optimization-hyperparameters)中来解决二元分类问题，从而对线性回归算法进行调整。sigmoid 函数将任何输入压缩成 0 到 1 之间的值。所得值可以解释为给定事件（在本例中为特定分类）发生的概率。

[**K 近邻** （KNN）](https://www.ibm.com/think/topics/knn) 算法基于数据点在[向量嵌入空间](https://www.ibm.com/think/topics/vector-embedding)中与其他已分类（即已标记）数据点的接近程度对其进行分类，其假设是相似的数据点彼此靠近。k 表示考虑的邻近数据点的*数量* ：例如，在 *k=5 的 KNN 算法中，* 输入数据点将与其 5 个最近邻进行比较，并根据这 5 个邻近数据点中出现频率最高的类别进行分类。

[**支持向量机 (SVM)**](https://www.ibm.com/think/topics/support-vector-machine) 是一种强大的模型，表面上用于执行二元分类，但也可以应用于多类分类问题。SVM 算法的目标并非直接学习如何对数据点进行分类，而是学习最佳*决策边界* ，以区分两类已标记的数据点，从而根据新数据点落在边界的哪一侧对其进行分类。SVM 算法将此边界定义为最大化相对类别数据点之间间隔（或间隙）的超平面，这一概念类似于[半监督学习](https://www.ibm.com/think/topics/semi-supervised-learning)中的*低密度假设* 。从逻辑上讲，只有每个类别中距离边界最近的数据点才能支持该超平面的计算。因此，这些边界相邻数据点的向量嵌入被称为*支持向量。*