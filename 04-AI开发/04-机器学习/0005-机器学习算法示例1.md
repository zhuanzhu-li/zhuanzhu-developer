<h1>机器学习算法</h1>

# 一、有监督学习

## 1. 线性回归

代码示例：

~~~python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import tkinter as tk
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# 设置中文字体，解决中文显示问题
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'KaiTi']  # Windows 常用中文字体
plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示为方块的问题

# 设置随机种子确保结果可重现
np.random.seed(42)

# 生成示例数据：房屋面积与价格关系
# 假设真实关系是 y = 3*X + 2 + 噪声
X = np.random.rand(100, 1) * 100  # 100个房屋面积数据，范围0-100平方米
y = 3 * X.flatten() + 2 + np.random.randn(100) * 10  # 房价，带有随机噪声

print(f"数据形状: X{X.shape}, y{y.shape}")
print("前5个样本:")
for i in range(5):
    print(f"房屋面积: {X[i][0]:.1f}平方米, 价格: {y[i]:.1f}万元")

# 划分训练集(80%)和测试集(20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建统一的大图，3行2列布局，增大高度确保不重叠
fig, axes = plt.subplots(3, 2, figsize=(14, 18))

# 图1: 房屋面积与价格关系
axes[0, 0].scatter(X_train, y_train, color='blue', alpha=0.7, label='训练集')
axes[0, 0].scatter(X_test, y_test, color='red', alpha=0.7, label='测试集')
axes[0, 0].set_xlabel('房屋面积 (平方米)')
axes[0, 0].set_ylabel('价格 (万元)')
axes[0, 0].set_title('1. 房屋面积与价格关系')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# 图2: 目标变量分布
axes[0, 1].hist(y_train, alpha=0.7, label='训练集', bins=15)
axes[0, 1].hist(y_test, alpha=0.7, label='测试集', bins=15)
axes[0, 1].set_xlabel('价格 (万元)')
axes[0, 1].set_ylabel('频数')
axes[0, 1].set_title('2. 目标变量分布')
axes[0, 1].legend()


# 创建线性回归模型
model = LinearRegression()

# 在训练集上训练模型
model.fit(X_train, y_train)

# 获取模型参数
slope = model.coef_[0]  # 系数(斜率)
intercept = model.intercept_  # 截距

print("=== 模型训练结果 ===")
print(f"回归方程: y = {slope:.4f} * X + {intercept:.4f}")
print(f"学习到的斜率(系数): {slope:.4f}")
print(f"学习到的截距: {intercept:.4f}")
print(f"真实关系: y = 3 * X + 2 + 噪声")

# 对比真实关系与学习到的关系
x_range = np.linspace(0, 100, 100)
y_learned = slope * x_range + intercept
y_true = 3 * x_range + 2

# 图3: 线性回归拟合结果
axes[1, 0].scatter(X_train, y_train, color='blue', alpha=0.7, label='训练数据')
axes[1, 0].plot(x_range, y_true, 'g--', label='真实关系: y=3x+2', linewidth=2)
axes[1, 0].plot(x_range, y_learned, 'r-', label=f'学习到的关系: y={slope:.2f}x+{intercept:.2f}', linewidth=2)
axes[1, 0].set_xlabel('房屋面积 (平方米)')
axes[1, 0].set_ylabel('价格 (万元)')
axes[1, 0].set_title('3. 线性回归拟合结果')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)


# 在测试集上进行预测
y_pred = model.predict(X_test)

# 计算评估指标
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("=== 模型测试集性能 ===")
print(f"均方误差 (MSE): {mse:.2f}")
print(f"均方根误差 (RMSE): {rmse:.2f}")
print(f"决定系数 (R²): {r2:.4f}")

# 图4: 预测值vs实际值
axes[1, 1].scatter(X_test, y_test, color='blue', alpha=0.7, label='真实值')
axes[1, 1].scatter(X_test, y_pred, color='red', alpha=0.7, label='预测值')
axes[1, 1].plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', label='完美预测')
axes[1, 1].set_xlabel('实际价格 (万元)')
axes[1, 1].set_ylabel('预测价格 (万元)')
axes[1, 1].set_title('4. 预测值 vs 实际值')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

# 图5: 残差分析
residuals = y_test - y_pred
axes[2, 0].scatter(y_pred, residuals, alpha=0.7)
axes[2, 0].axhline(y=0, color='red', linestyle='--')
axes[2, 0].set_xlabel('预测值')
axes[2, 0].set_ylabel('残差 (实际值-预测值)')
axes[2, 0].set_title('5. 残差分析')
axes[2, 0].grid(True, alpha=0.3)

# 输出部分预测结果对比
print("\n=== 预测结果对比 (前10个样本) ===")
print("面积(㎡)\t实际价格(万)\t预测价格(万)\t误差")
print("-" * 55)
for i in range(min(10, len(X_test))):
    error = y_pred[i] - y_test[i]
    print(f"{X_test[i][0]:.1f}\t{y_test[i]:.2f}\t\t{y_pred[i]:.2f}\t\t{error:+.2f}")

# 使用模型进行新预测
new_areas = np.array([[45], [78], [120], [150]])  # 新房屋面积
predictions = model.predict(new_areas)

print("=== 新房屋价格预测 ===")
for area, price in zip(new_areas, predictions):
    print(f"房屋面积 {area[0]:.0f} 平方米 -> 预测价格: {price:.2f} 万元")

# 图6: 新房屋价格预测
axes[2, 1].scatter(X, y, alpha=0.5, label='历史数据')
axes[2, 1].scatter(new_areas, predictions, color='red', s=100, marker='*', label='新预测')
axes[2, 1].plot(x_range, slope * x_range + intercept, 'r-', label='回归直线')
axes[2, 1].set_xlabel('房屋面积 (平方米)')
axes[2, 1].set_ylabel('价格 (万元)')
axes[2, 1].set_title('6. 新房屋价格预测')
axes[2, 1].legend()
axes[2, 1].grid(True, alpha=0.3)

# 统一显示所有图表
fig.suptitle('线性回归完整分析', fontsize=16, fontweight='bold', y=0.995)
plt.subplots_adjust(left=0.08, right=0.95, top=0.96, bottom=0.05, hspace=0.35, wspace=0.25)

# 创建带滚动条的 tkinter 窗口
root = tk.Tk()
root.title('线性回归完整分析')
root.geometry('1200x700')  # 窗口初始大小

# 创建主框架
main_frame = tk.Frame(root)
main_frame.pack(fill=tk.BOTH, expand=True)

# 创建画布和滚动条
canvas = tk.Canvas(main_frame)
scrollbar_y = tk.Scrollbar(main_frame, orient=tk.VERTICAL, command=canvas.yview)
scrollbar_x = tk.Scrollbar(root, orient=tk.HORIZONTAL, command=canvas.xview)

# 配置画布滚动
canvas.configure(yscrollcommand=scrollbar_y.set, xscrollcommand=scrollbar_x.set)

# 布局滚动条和画布
scrollbar_y.pack(side=tk.RIGHT, fill=tk.Y)
scrollbar_x.pack(side=tk.BOTTOM, fill=tk.X)
canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

# 创建内部框架用于放置图表
inner_frame = tk.Frame(canvas)
canvas.create_window((0, 0), window=inner_frame, anchor='nw')

# 将 matplotlib 图表嵌入到 tkinter 窗口
figure_canvas = FigureCanvasTkAgg(fig, master=inner_frame)
figure_canvas.draw()
figure_canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

# 更新滚动区域
def update_scroll_region(event=None):
    canvas.configure(scrollregion=canvas.bbox('all'))

inner_frame.bind('<Configure>', update_scroll_region)

# 添加鼠标滚轮支持
def on_mousewheel(event):
    canvas.yview_scroll(int(-1 * (event.delta / 120)), 'units')

canvas.bind_all('<MouseWheel>', on_mousewheel)

# 运行 tkinter 主循环
root.mainloop()
~~~

![image-20251208101052687](0005-机器学习算法示例1.assets/image-20251208101052687.png)

![image-20251208101108583](0005-机器学习算法示例1.assets/image-20251208101108583.png)

![image-20251208101119969](0005-机器学习算法示例1.assets/image-20251208101119969.png)

## 2. LDA & QDA

线性判别分析（LDA）和二次判别分析（QDA）是两种基于概率模型的高效分类算法

线性判别分析（LDA）和二次判别分析（QDA）是两种基于概率模型的高效分类算法。下面我将为你详细解析它们的作用、使用场景，并提供一个具体的代码示例。

### 算法概述与核心作用

LDA和QDA都是经典的统计学习方法，基于数据服从高斯分布的假设，通过贝叶斯定理构建分类决策边界。

**核心作用**：

- **分类任务**：将样本划分到不同的预定义类别中
- **维度压缩**：LDA可执行有监督的降维，找到最大化类间分离的方向
- **概率输出**：提供样本属于各类别的后验概率估计

### 主要特性

| 特性           | 线性判别分析 (LDA)         | 二次判别分析 (QDA)         |
| -------------- | -------------------------- | -------------------------- |
| **决策边界**   | 线性                       | 二次曲线                   |
| **协方差假设** | 所有类别共享相同协方差矩阵 | 每个类别有自己的协方差矩阵 |
| **参数数量**   | 较少（更简单）             | 较多（更灵活）             |
| **适用场景**   | 数据近似线性可分           | 数据需要非线性决策边界     |
| **过拟合风险** | 较低                       | 较高（尤其小样本时）       |

### 示例代码

~~~python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis

# 生成合成数据
def generate_data(cov_class_1, cov_class_2, n_samples=300):
    rng = np.random.RandomState(0)
    X1 = rng.randn(n_samples, 2) @ cov_class_1
    X2 = rng.randn(n_samples, 2) @ cov_class_2 + np.array([1, 1])
    X = np.concatenate([X1, X2])
    y = np.concatenate([np.zeros(n_samples), np.ones(n_samples)])
    return X, y

def plot_decision_boundary(ax, clf, X, y, title):
    # 创建网格
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                         np.arange(y_min, y_max, 0.02))
    
    # 预测网格点的类别
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    # 绘制轮廓和数据点
    ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k', s=25)
    ax.set_title(title)

#  scenario 1: 相同协方差矩阵（LDA理想场景）
covariance = np.array([[0.0, -0.23], [0.83, 0.23]])
X1, y1 = generate_data(covariance, covariance)

#  scenario 2: 不同协方差矩阵（QDA理想场景）
cov_class_1 = np.array([[0.0, -1.0], [2.5, 0.7]]) * 2.0
cov_class_2 = cov_class_1.T  # 转置矩阵，结构不同
X2, y2 = generate_data(cov_class_1, cov_class_2)

# 训练并比较模型
lda = LinearDiscriminantAnalysis()
qda = QuadraticDiscriminantAnalysis()

fig, axes = plt.subplots(2, 2, figsize=(10, 8))

for i, (X, y) in enumerate([(X1, y1), (X2, y2)]):
    lda.fit(X, y)
    qda.fit(X, y)
    
    plot_decision_boundary(axes[i, 0], lda, X, y, f"Dataset {i+1} : LDA")
    plot_decision_boundary(axes[i, 1], qda, X, y, f"Dataset {i+1} : QDA")

plt.tight_layout()
plt.show()
~~~

![image-20251209162256835](0005-机器学习算法示例1.assets/image-20251209162256835.png)